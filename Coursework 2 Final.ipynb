{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf432e3-ab9b-43fa-b7ec-bde01b5ea900",
   "metadata": {
    "id": "bbf432e3-ab9b-43fa-b7ec-bde01b5ea900"
   },
   "source": [
    "# Coursework 2: Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2786210f-124e-4482-9c62-f1786c0bdad5",
   "metadata": {
    "id": "2786210f-124e-4482-9c62-f1786c0bdad5"
   },
   "source": [
    "This is the second coursework of ECS7023P Programming for AI and Data Science, which counts 65% towards the final grade of the module. The coursework is graded out of 100 marks.\n",
    "\n",
    "**Deadline:** Thursday 2nd November, 2023 - 11.59pm\n",
    "\n",
    "**Marking criteria:** While the most important marking criterion will be for the code to achieve the expected objective and output, marks will also be given for partial or close solutions, whereas marks can be deducted for code that is overly complex, inefficient, difficult to understand and/or to maintain.\n",
    "\n",
    "**Use of packages:** In addition to built-in python functions and elements that we have seen in the lectures (see lecture notes), the only additional packages allowed for this exercise include *string* and *json*.\n",
    "\n",
    "**How to submit:** You will submit a completed Jupyter notebook file with your solutions, as well as a PDF version of the Jupyter notebook which includes the outputs of your code (either two separate files or a single zip file that contains both the .ipynb and .pdf files). Your submission will include the python code that produces the required answers. Answers produced through means other than python code will not be deemed acceptable.\n",
    "\n",
    "**Note:** This is an individual coursework, the solutions you submit need to be your own and developed on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8ca95-d2a7-4fc4-a0b3-dac510f1c74c",
   "metadata": {
    "id": "81a8ca95-d2a7-4fc4-a0b3-dac510f1c74c"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a558d85-b727-4afb-a832-04696150b89a",
   "metadata": {
    "id": "7a558d85-b727-4afb-a832-04696150b89a"
   },
   "source": [
    "For this exercise, you are given a dataset that contains a sample of Wikipedia articles, with their content reduced to only text (that is with images and other non-textual content removed).\n",
    "\n",
    "The dataset is contained in a file called 'wiki-articles.json', where each line contains an entry with a Wikipedia article: an ID, a URL, title and body text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21d512-f047-4fa0-9e3b-6436d0e2c96e",
   "metadata": {
    "id": "fa21d512-f047-4fa0-9e3b-6436d0e2c96e"
   },
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e98386-ca69-478c-b575-cd953b34263d",
   "metadata": {
    "id": "b4e98386-ca69-478c-b575-cd953b34263d"
   },
   "source": [
    "**Note:** For this coursework, you should remove all the punctuation prior to processing the text. To do this, use the following code, where *text* is the variable where you want to remove the punctuation:\n",
    "\n",
    "```\n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "text = text.translate(translator)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e549a-a74e-47ad-9f96-1bf269996aac",
   "metadata": {
    "id": "9b4e549a-a74e-47ad-9f96-1bf269996aac"
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e129ea7-3712-4cf7-85c9-d465f6504d3b",
   "metadata": {
    "id": "1e129ea7-3712-4cf7-85c9-d465f6504d3b"
   },
   "source": [
    "1. Ignoring the case, what is the most frequently occurring word across all articles in the dataset? Multiple occurrences of the same word within an article should only be counted once. **(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309512b2-60e6-477c-acc0-4678342bd6c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b3c8805-eea5-4415-9487-eb586e270d14",
    "outputId": "1a2478fb-ccbd-418e-8e08-73d72efdae0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequently occurring word is: the, and the count is: 19227\n",
      "CPU times: user 55.8 s, sys: 1.09 s, total: 56.9 s\n",
      "Wall time: 59.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "import string\n",
    "\n",
    "# Function created to load the articles\n",
    "def load_wiki_articles(file_path):\n",
    "    articles = []\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            entry = json.loads(line)\n",
    "            articles.append(entry)\n",
    "    return articles\n",
    "\n",
    "# Function created to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_no_punctuation = text.translate(translator)\n",
    "    text_lowercase = text_no_punctuation.lower()\n",
    "    return text_lowercase\n",
    "\n",
    "# Load articles\n",
    "wiki_articles = load_wiki_articles('wiki-articles.json')\n",
    "\n",
    "# Count word occurrences\n",
    "word_count = {}\n",
    "for article in wiki_articles:\n",
    "    text = article['text']\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    words = preprocessed_text.split()\n",
    "    set_words = set(words)\n",
    "    for word in set_words:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "# Find the most common word\n",
    "most_common_word = max(word_count, key=word_count.get)\n",
    "\n",
    "# Print the most frequently occurring word\n",
    "print(f\"The most frequently occurring word is: {most_common_word}, and the count is: {word_count[most_common_word]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6da2c-a342-4908-997c-7253095f8bfe",
   "metadata": {
    "id": "7ac6da2c-a342-4908-997c-7253095f8bfe"
   },
   "source": [
    "2. The least frequent characters in English are: x, z, j, q. How many articles are there in the dataset that contain all these 4 characters (at least one occurrence of each)? **(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0a822d-348c-45b9-8999-e64158c2884e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "166e2ea4-eaf8-4b76-a268-32461ad51577",
    "outputId": "3cd6ca6b-fccd-483d-bd8c-12fcef455544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles containing all four least frequent characters: 14688\n",
      "CPU times: user 32.3 s, sys: 103 ms, total: 32.4 s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Function created to find the number of articles containing the least frequent characters\n",
    "def articles_containing_least_frequent_chars(dataset, least_frequent_chars):\n",
    "    articles_with_least_frequent_chars = 0\n",
    "\n",
    "    for article in dataset:\n",
    "        text = article['text']\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "\n",
    "        # Check if all least frequent characters are present\n",
    "        has_all_chars = all(char in preprocessed_text for char in least_frequent_chars)\n",
    "        \n",
    "        if has_all_chars:\n",
    "            articles_with_least_frequent_chars += 1\n",
    "\n",
    "    return articles_with_least_frequent_chars\n",
    "\n",
    "# Least frequent characters as a set\n",
    "least_frequent_chars = set(['x', 'z', 'j', 'q'])\n",
    "\n",
    "# Count the articles containing all four least frequent characters using the revised function above\n",
    "count_least_frequent_chars = articles_containing_least_frequent_chars(wiki_articles, least_frequent_chars)\n",
    "\n",
    "# Output the count of articles containing all four least frequent characters\n",
    "print(f\"Number of articles containing all four least frequent characters: {count_least_frequent_chars}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44505c1b-ebd2-42ac-9ad1-a1a98cda7bba",
   "metadata": {
    "id": "44505c1b-ebd2-42ac-9ad1-a1a98cda7bba"
   },
   "source": [
    "3. What is the longest alphabetical word in the dataset that starts with the letter 'p', and what is the length of this word? (alphabetical word = containing only a-z letters and no numbers or other characters) **(10 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0ff443-e562-4f78-be1f-9422266b953e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de4b3c1f-f5a0-4133-9e29-289b25f14780",
    "outputId": "53c0e83b-e14f-4692-bd8a-bb28ad4c998d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest alphabetical word in the dataset that starts with the letter 'p' is 'pneumonoultramicroscopicsilicovolcanoconiosis' with a length of 45 characters.\n",
      "CPU times: user 46.1 s, sys: 211 ms, total: 46.3 s\n",
      "Wall time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Function created to find the longest_alphabetical word starting with p\n",
    "def longest_alphabetical_word_starting_with_p(dataset):\n",
    "    longest_word = ''\n",
    "    longest_word_length = 0\n",
    "\n",
    "    for article in dataset:\n",
    "        text = article['text']\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        words = preprocessed_text.split()\n",
    "        \n",
    "        for word in words:\n",
    "            if word.startswith('p') and word.isalpha():\n",
    "                if len(word) > longest_word_length:\n",
    "                    longest_word = word\n",
    "                    longest_word_length = len(word)\n",
    "\n",
    "    return longest_word, longest_word_length\n",
    "\n",
    "# Find the longest word starting with 'p' in the dataset...\n",
    "longest_p_word, longest_p_word_length = longest_alphabetical_word_starting_with_p(wiki_articles)\n",
    "\n",
    "# Output the longest word starting with 'p' and its lenght...\n",
    "print(f\"The longest alphabetical word in the dataset that starts with the letter 'p' is '{longest_p_word}' with a length of {longest_p_word_length} characters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1fd8a8-5c81-46e1-96ad-8ecc2acfb367",
   "metadata": {
    "id": "bd1fd8a8-5c81-46e1-96ad-8ecc2acfb367"
   },
   "source": [
    "4. What is the longest sequence of capitalised words in the dataset? Only consider if the first character of each word is upper case, regardless of whether the rest of the characters are upper or lower case. **(15 marks)**\n",
    "\n",
    "**Toy example:** if we had the following sentence: \"Queen Mary University is located in the United Kingdom, specifically in London\". This example has 3 sequences of capitalised words: Queen Mary University, United Kingdom and London. Of these, the longest sequence of capitalised words is Queen Mary University, with 3 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cba9ee-239e-4f4d-86b8-dd57380ea1c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13d9cb29-6f39-46d1-af80-c30e5de96b09",
    "outputId": "32dacef4-9769-4bc1-8b48-445399ee54c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Sequence: Havilland, Michael Curtiz, James Cagney, David O. Selznick, William Wyler, Richard Brooks, Harry Cohn, Jane Wyman, Jean Arthur, Claude Rains, Raymond Massey, George Raft, Myrna Loy, Lee J. Cobb, Gene Kelly, Henry Fonda, John Wayne, Jimmy Stewart, Jack Benny, Robert Taylor, Eleanor Parker, Greer Garson, Bing Crosby, Ronald Colman, Lena Horne, Joan Crawford, Marilyn Monroe, Ingrid Bergman, Glenda Farrell, Don Ameche, Ann Sheridan, Ida Lupino, Joan Blondell, Alexander Knox, Veronica Lake, Randolph Scott, Miriam Hopkins, José Ferrer, Charles Laughton, Mary Astor, Bruce Bennett, Margaret Lindsay, Sylvia Sidney, Alexis Smith, Priscilla Lane, Mary Pickford, Ralph Bellamy, Cyd Charisse, Cesar Romero, Ann Sothern, Zero Mostel, Walter Brennan, Jennifer Jones, Louella Parsons, Joel McCrea, Norma Shearer, John Huston, Agnes Moorehead, Rosalind Russell, Adolphe Menjou, Fredric March, Errol Flynn, Edward G. Robinson, Gregory Peck, Gary Cooper, Billy Wilder,\n",
      "Length: 134\n",
      "CPU times: user 43.8 s, sys: 130 ms, total: 44 s\n",
      "Wall time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def longest_sequence_capitalised_words(dataset):\n",
    "    longest_sequence = []\n",
    "    current_sequence = []\n",
    "\n",
    "    for article in dataset:\n",
    "        text = article['text']\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text_no_punctuation = text.translate(translator)\n",
    "        words = text.split()\n",
    "\n",
    "        for word in words:\n",
    "            if word[0].isupper():\n",
    "                current_sequence.append(word)\n",
    "            else:\n",
    "                if len(current_sequence) > len(longest_sequence):\n",
    "                    longest_sequence = current_sequence \n",
    "                current_sequence = []\n",
    "\n",
    "    # Check if the last sequence was the longest in the article\n",
    "    if len(current_sequence) > len(longest_sequence):\n",
    "        longest_sequence = current_sequence\n",
    "\n",
    "    return longest_sequence\n",
    "\n",
    "longest_capitalised_sequence = longest_sequence_capitalised_words(wiki_articles)\n",
    "longest_sequence = ' '.join(longest_capitalised_sequence)\n",
    "print(f'Longest Sequence: {longest_sequence}')\n",
    "print(f'Length: {len(longest_capitalised_sequence)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe5ca2-bd7a-4feb-ba33-16baec16ca68",
   "metadata": {
    "id": "63fe5ca2-bd7a-4feb-ba33-16baec16ca68"
   },
   "source": [
    "5. The same word can be written using different case (i.e. variations of lower and upper letters). For example, 'house' and 'HouSe' use the same letters to form the exact same word but with different case. Write the python code that identifies the word in the entire dataset that has the largest number of case variations. How many variations does it have? **(15 marks)**\n",
    "\n",
    "**Toy example:** if our dataset had only the following words: \"HouSe, House, HOUSE, HOUSE, YES, yes, YeS, Yes, yeS, yEs, yEs.\" We would then identify that 'house' has 3 variations and 'yes' has 6 variations. Therefore, 'yes' is the word with the largest number of variations, with 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef7d71e9-7dab-440d-ac42-9cef9cc752b5",
   "metadata": {
    "id": "22ca31bf-696c-4ec7-a044-33c953f54496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word in the entire dataset that has the largest number of case variations: pops, and the number of variations is: 6\n",
      "CPU times: user 57.6 s, sys: 409 ms, total: 58 s\n",
      "Wall time: 58.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def word_with_most_case_variations(dataset):\n",
    "    case_variations = {}\n",
    "\n",
    "    for article in dataset:\n",
    "        text = article['text']\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text_no_punctuation = text.translate(translator)\n",
    "        words = text_no_punctuation.split()  \n",
    "\n",
    "        for word in words:\n",
    "            standardised_word = word.lower()\n",
    "            if standardised_word not in case_variations:\n",
    "                case_variations[standardised_word] = {word}\n",
    "            else:\n",
    "                case_variations[standardised_word].add(word)\n",
    "\n",
    "    variation_count = 0\n",
    "    word = ''\n",
    "\n",
    "    for key, value in case_variations.items():\n",
    "        if len(value) > variation_count:\n",
    "            variation_count = len(value)\n",
    "            word = key\n",
    "\n",
    "    return word, variation_count\n",
    "\n",
    "# Example usage\n",
    "word, variations = word_with_most_case_variations(wiki_articles)\n",
    "print(f'The word in the entire dataset that has the largest number of case variations: {word}, and the number of variations is: {variations}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1e5c7-5065-4756-ac6b-55315a0da2b1",
   "metadata": {
    "id": "aee1e5c7-5065-4756-ac6b-55315a0da2b1"
   },
   "source": [
    "6. Two words are anagrams of each other if they can be written using the same characters but in a different order, including repeated characters. For example, 'cheap' is an anagram of 'peach', 'reacting' is an anagram of 'creating' and 'resistance' is an anagram of 'ancestries'.\n",
    "\n",
    "Likewise, **n-word anagram sets** include **n** words that are anagrams of one another:\n",
    "- 'asleep', 'please' and 'elapse' form a 3-word anagram set.\n",
    "- 'aspired', 'despair', 'diapers' and 'praised' form a 4-word anagram set.\n",
    "\n",
    "By only considering words that are *6 characters or longer*, list all the anagram sets of 4 words or more (that is n-word anagram sets where n >= 4) that can be found within any article, i.e. we are not interested in checking anagrams across articles. **(15 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c060fa86-4085-4103-a2a6-c00899489f6e",
   "metadata": {
    "id": "c060fa86-4085-4103-a2a6-c00899489f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'siphlodora', 'drosophila', 'phloridosa', 'lordiphosa', 'dorsilopha', 'psilodorha'}\n",
      "{'vftqsbporuzwyxhgdiecjalnmk', 'xdybpwosmuzriqgenlhvjtfack', 'jsrhfenduazyqgxtmcbpiwvolk', 'ofrjvmazhqnbxpykculgswetdi', 'fcbjqawtvdynxlusezphoigmkr', 'kptxigfmesauhyqbovjclrzdnw', 'xjmiyvcarqowhlndsufkgbepzt', 'rcbutxvzjinqpkwmlayedgofsh', 'zgvrkobxlneiwjfusdqypcmhta', 'tljrvqhgucxbzyswfdoaiepknm', 'rcbpqmvzxyuofsldeanwkgtijh', 'hvgpwsumdbtncokxjiqzrflaey', 'ceakbmryuvdnfltxwgzoijqphs', 'jkgoptcihabrnmdeylzfxwvuqs', 'fsktjarxpecnulyizgbdmwvhoq', 'jiozfewmbaushpcnrqlvktgyxd', 'zqxuvgfnwrlkphtmbjyodeicsa', 'dliajuovcexbnmgqpwzyfhrkts', 'efmqabguinkxcjordpzthwvlys', 'hlnrskjamgfbicuqpdeyozxwtv', 'nukchvsmdgtzqfyewpialoxrjb', 'xjwfrdzsqblktvpoiehmyncaug', 'gcbuzrasyxvmlpqnofhwdktjie', 'urfxncmylvpigesktboqajzdhw', 'uorytqslwxzhnmbvfcgeapijdk', 'xpjuowiygcvrtqebnlzmdkfahs', 'ktjuqonpzcamlgfhewxbdyrsvi', 'gkqrfeanzpbmlhvjcduxsoytwi', 'qungalxepkzyrdsoftvcmbihwj', 'krulgjewnfadvipoybxzcmhsqt', 'glqywbtizdpsfkanjcuxrevmoh', 'pdsbtiuqfnovwjkahzceglmyxr', 'lushqoxdmznaikfrepcybwvgtj', 'abcdefghijklmnopqrstuvwxyz', 'kgwntrblqpahydvjifxezocsmu', 'disauyombpnthkgjrqclezxwfv', 'rdobjntkvehmlfcwzaxgyipsuq', 'ymztgvekqohpbsjliundrfxwac', 'rmjvlyqzkciebonugawxpdstfh', 'fjlvqakxnbgcpirmeoyzwduhst', 'tzdipnjesycuhavrmxgkbfqwol', 'evtnhqdxwzjfucpiamorbsyglk', 'yhlpgtebkwicsvudrqmfonjzax'}\n",
      "{'integral', 'altinger', 'triangle', 'relating'}\n",
      "CPU times: user 1min 15s, sys: 695 ms, total: 1min 15s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "with open('wiki-articles.json', 'r') as wiki_file:\n",
    "    for article in wiki_file:\n",
    "        entry = json.loads(article)\n",
    "        text = entry['text'].translate(str.maketrans('', '', string.punctuation))\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        anagrams = {}\n",
    "        \n",
    "        for word in words:\n",
    "    \n",
    "            if len(word) >= 6 and word.isalpha():\n",
    "                sorted_word = ''.join(sorted(word))\n",
    "                if sorted_word not in anagrams:\n",
    "                    anagrams[sorted_word] = set()\n",
    "                anagrams[sorted_word].add(word)\n",
    "         \n",
    "        anagram_sets = [words for words in anagrams.values() if len(words) >= 4]\n",
    "\n",
    "        for anagram_set in anagram_sets:\n",
    "                print(anagram_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98b7407-13e9-4490-9eb9-709bb4bcacf6",
   "metadata": {
    "id": "f98b7407-13e9-4490-9eb9-709bb4bcacf6"
   },
   "source": [
    "7. With the aim of building a small search engine, we want to index the contents of the articles in the dataset (i.e. mapping which articles contain each word), where we also want to store the position(s) in which content appears in each article. For example, if an article ID is 7 and its text is \"python coding is so much fun\", we would say that the word 'coding' appears in position 1 of article ID 7 and the word 'fun' appears in position 5 of article ID 7. A toy example of an index could be:\n",
    "- 'house' can be found in: position 97 of article 7, positions 32 and 221 of article 13, and position 63 of article 27\n",
    "- 'London' can be found in: positions 17 and 97 of article 7, position 21 of article 25, and position 157 of article 42.\n",
    "- etc.\n",
    "\n",
    "**(35 marks total)** (see specific marks for each of the subquestions of question 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44423db-3127-4192-88a8-9dbf5369bbc5",
   "metadata": {
    "id": "a44423db-3127-4192-88a8-9dbf5369bbc5"
   },
   "source": [
    "7. (a) Create a data structure that indexes the dataset contents following the above objective. You should ignore the case (i.e. lowercase everything) for this exercise. **(15 marks)**\n",
    "\n",
    "**Note:** questions 7b and 7c need to be implemented using the indexing structure created here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b5bdc66-09af-4222-ac1f-0899ba76331d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "bac89e55-a8e7-4738-a2a2-13f9093003fe",
    "outputId": "fe43b8ec-41a8-42f2-d13d-1eba98e1e588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 5.75 s, total: 1min 50s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def index(dataset):\n",
    "    word_index = {}\n",
    "\n",
    "    for article in dataset:\n",
    "        article_id = article['id']\n",
    "        text = article['text']\n",
    "        processed_text = preprocess_text(text)\n",
    "        words = processed_text.split()\n",
    "\n",
    "        position = 0\n",
    "        for word in words:\n",
    "            position += 1\n",
    "            if word not in word_index:\n",
    "                word_index[word] = {}\n",
    "            if article_id not in word_index[word]:\n",
    "                word_index[word][article_id] = []\n",
    "            word_index[word][article_id].append(position)\n",
    "\n",
    "    return word_index\n",
    "\n",
    "resulting_index = index(wiki_articles) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92192361-5a68-4a30-9f70-19e02e13ea5b",
   "metadata": {
    "id": "92192361-5a68-4a30-9f70-19e02e13ea5b"
   },
   "source": [
    "7. (b) Given a word as input parameter, write a function that outputs the article IDs in which the word can be found. As an example, produce the output for the word 'python'. **(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d748a1ab-8bd6-48da-86f1-273c54a4b176",
   "metadata": {
    "id": "d748a1ab-8bd6-48da-86f1-273c54a4b176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article IDs where 'python' can be found: ['594', '809', '1063', '1437', '2581', '2807', '3054', '3382', '4015', '4147', '4373', '4579', '4653', '4668', '4706', '4887', '4893', '5213', '5644', '5700', '5739', '6021', '6667', '6696', '6698', '6974', '7220', '7392', '7574', '7575', '7951', '7962', '8091', '8267', '8531', '8786', '8829', '8904', '8996', '9010', '9069', '9288', '9498', '9710', '9838', '10049', '10283', '10474', '10606', '10793', '10933', '11142', '11168', '11367', '11376', '11642', '11673', '11741', '11966', '12302', '12531', '12628', '12731', '13024', '13052', '13208', '13790', '13834', '14467', '14794', '14801', '15154', '15305', '15704', '15858', '16036', '16389', '16708', '16808', '17399', '17415', '17443', '17871', '17920', '18016', '18137', '18155', '18203', '18574', '18692', '18838', '18894', '18942', '18977', '19160', '19161', '19162', '19163', '19164', '19165', '19545', '19550', '19620', '19669', '19701', '20034', '20036', '20039', '20362', '20412', '21017', '21037', '21506', '21523', '21796', '21873', '22055', '22091', '22145', '22330', '22589', '22592', '22626', '22660', '22693', '22758', '22826', '23015', '23329', '23529', '23547', '23659', '23716', '23824', '23862', '23939', '24131', '24185', '24267', '24518', '24522', '24722', '24894', '25184', '25270', '25407', '25409', '25717', '25739', '25768', '25794', '26386', '26490', '27027', '27695', '27701', '27977', '28119', '28319', '28341', '28367', '28368', '28442', '28555', '28698', '28760', '28804', '28938', '29004', '29208', '29370', '29519', '29549', '29843', '30054', '30302', '30410', '30816', '30890', '31139', '31180', '31206', '31268', '31454', '31573', '31742', '32188', '32478', '33164', '33898', '33955', '33958', '34004', '34119', '34138', '34211', '34358', '34472', '36713', '36777', '37035', '37472', '37891', '38221', '38689', '38838', '39289', '40132', '40317', '41227']\n",
      "210\n",
      "CPU times: user 344 µs, sys: 206 µs, total: 550 µs\n",
      "Wall time: 407 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def find_word(word, word_index):\n",
    "    word = word.lower()  # Ensure the word is in lowercase so we are comparing lowercase to lowercase\n",
    "    if word in word_index:\n",
    "        return list(word_index[word].keys())\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Find the article IDs for the word 'python'\n",
    "word_to_search = 'python'\n",
    "article_ids_for_word = find_word(word_to_search, resulting_index)\n",
    "print(f\"Article IDs where '{word_to_search}' can be found: {article_ids_for_word}\") \n",
    "print(len(article_ids_for_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666603b4-6856-4e00-a6ae-4cff1fe8a30c",
   "metadata": {
    "id": "666603b4-6856-4e00-a6ae-4cff1fe8a30c"
   },
   "source": [
    "7. (c) Given two words and a distance value (integer) as input, return the list of article IDs where the two words occur within the distance. As an example, produce the output for words 'python' and 'coding' within a distance of 20. **(15 marks)**\n",
    "\n",
    "**For example,** if we have a dataset with five articles:\n",
    "- article ID 1: \"python coding\"\n",
    "- article ID 2: \"I am coding in python\"\n",
    "- article ID 3: \"To learn coding, I have decided to start with python\"\n",
    "- article ID 4: \"python is a fantastic programming language\"\n",
    "- article ID 5: \"it's sunny today and I like it\"\n",
    "\n",
    "If we are given two words: 'python' and 'coding', and a distance of 2:\n",
    "- article ID 1: the condition is true as 'python' and 'coding' occur within a distance of 1.\n",
    "- article ID 2: the condition is true as 'python' and 'coding' occur within a distance of 2.\n",
    "- article ID 3: the condition is false as 'python' and 'coding' occur within a larger distance of 7.\n",
    "- article ID 4: the condition is false because it only contains 'python', no 'coding'.\n",
    "- article ID 5: the condition is false because it doesn't contain any of the two keywords 'python' and 'coding'.\n",
    "\n",
    "Therefore, in this case, within_distance_ids('python', 'coding', 2) would output [1, 2] as the list of matching article IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160ba33d-7e14-4a82-96a2-7f53cfa67813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article IDs where 'python' and 'coding' occur within a distance of 20: ['23862', '6974']\n",
      "CPU times: user 691 µs, sys: 354 µs, total: 1.05 ms\n",
      "Wall time: 1.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def find_words_within_distance(word1, word2, distance, word_index):\n",
    "    word1 = word1.lower()\n",
    "    word2 = word2.lower()\n",
    "\n",
    "    articles_with_both_words = set()\n",
    "\n",
    "    if word1 in word_index and word2 in word_index:\n",
    "        for article_id in word_index[word1]:\n",
    "            if article_id in word_index[word2]:\n",
    "                positions_word1 = word_index[word1][article_id]\n",
    "                positions_word2 = word_index[word2][article_id]\n",
    "\n",
    "                for pos1 in positions_word1:\n",
    "                    for pos2 in positions_word2:\n",
    "                        if abs(pos1 - pos2) <= distance:\n",
    "                            articles_with_both_words.add(article_id)\n",
    "\n",
    "    return list(articles_with_both_words)\n",
    "\n",
    "# Example usage for 'python' and 'coding' within a distance of 20\n",
    "word1_to_search = 'python'\n",
    "word2_to_search = 'coding'\n",
    "distance_value = 20\n",
    "\n",
    "article_ids_for_words_within_distance = find_words_within_distance(word1_to_search, word2_to_search, distance_value, resulting_index)\n",
    "print(f\"Article IDs where '{word1_to_search}' and '{word2_to_search}' occur within a distance of {distance_value}: {article_ids_for_words_within_distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffee07-8c0c-4690-b95f-cc7d068b32c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
